{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Project Code\"\n",
    "subtitle: Team Saturn\n",
    "author: Kaitlyn Hung, Amy Wang, Anastasia Wei, and Lila Wells \n",
    "date: 03/13/2023\n",
    "number-sections: true\n",
    "abstract: _This file contains the code for the project on <Exploring the Relationship Between Hospital Stays and the Readmission of Diabetic Patients>, as part of the STAT 303-2 course in Winter 2023_.\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5ee91",
   "metadata": {},
   "source": [
    "## Length of the code {-}\n",
    "No restriction\n",
    "\n",
    "**Delete this section from the report, when using this template.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "## Data quality check / cleaning / preparation \n",
    "\n",
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.** An example is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db149d8b",
   "metadata": {},
   "source": [
    "### Data quality check\n",
    "*By Elton John*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc82e7f",
   "metadata": {},
   "source": [
    "The code below visualizes the distribution of all the variables in the dataset, and their association with the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading relevant libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Loading the data\n",
    "data = pd.read_csv(\"dataset_diabetes/diabetic_data.csv\")\n",
    "ID = pd.read_csv('dataset_diabetes/IDs_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5955618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Distribution of continuous variables...#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8faafdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Distribution of categorical variables...#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e389f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Association of the response with the predictors...#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1561829",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "*By Anastasia Wei*\n",
    "\n",
    "From the data quality check we realized that:\n",
    "\n",
    "1. Some of the columns that should have contained only numeric values, specifically <>, <>, and <> have special characters such as \\*, #, %. We'll remove these characters, and convert the datatype of these columns to numeric.\n",
    "\n",
    "2. Some of the columns have more than 60% missing values, and it is very difficult to impute their values, as the values seem to be missing at random with negligible association with other predictors. We'll remove such columns from the data.\n",
    "\n",
    "3. The column `number_of_bedrooms` has some unreasonably high values such as 15. As our data consist of single-family homes in Evanston, we suspect that any value greater than 5 may be incorrect. We'll replace all values that are greater than 5 with an estimate obtained using the $K$-nearest neighbor approach.\n",
    "\n",
    "4. The columns `house_price` has some unreasonably high values. We'll tag all values greater than 1 billion dollars as \"potentially incorrect observation\", to see if they distort our prediction / inference later on.\n",
    "\n",
    "The code below implements the above cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f73626",
   "metadata": {},
   "outputs": [],
   "source": [
    "######-----------Changing the IDs into a three column format------------#########\n",
    "IDs = pd.DataFrame(index = range(63), columns = ['ID_types', 'ID_num', 'Description'])\n",
    "\n",
    "IDs.loc[:8, 'ID_types'] = ['admission_type_id'] * 9\n",
    "IDs.loc[:8, 'ID_num'] = ID.loc[:8,'admission_type_id'].values\n",
    "IDs.loc[:8, 'Description'] = ID.loc[:8,'description'].values\n",
    "\n",
    "IDs.loc[8:38, 'ID_types'] = ['discharge_disposition_id'] * 31\n",
    "IDs.loc[8:38, 'ID_num'] = ID.loc[10:40,'admission_type_id'].values\n",
    "IDs.loc[8:38, 'Description'] = ID.loc[10:40,'description'].values\n",
    "\n",
    "IDs.loc[38:, 'ID_types'] = ['admission_source_id'] * 25\n",
    "IDs.loc[38:, 'ID_num'] = ID.loc[42:,'admission_type_id'].values\n",
    "IDs.loc[38:, 'Description'] = ID.loc[42:,'description'].values\n",
    "\n",
    "# Saving the cleaned IDs to a new csv file\n",
    "IDs.to_csv('IDs_clean.csv')\n",
    "\n",
    "######-----------Dropping the columns w/ more than 50% values missing------------#########\n",
    "data.drop(['weight','medical_specialty'], axis = 1, inplace = True)\n",
    "\n",
    "######-----------Removing duplicate records for the same patient------------#########\n",
    "print('Length before removing Duplicates:', len(data))\n",
    "data.drop_duplicates(['patient_nbr'], keep = 'first', inplace = True)\n",
    "print('Length after removing Duplicates:', len(data))\n",
    "\n",
    "######-----------Changing readmission to two levels instead of three------------#########\n",
    "\n",
    "# Checking the values of the readmitted column and changing it to two levels\n",
    "# Readmitted is defined here as '1' if the patient returns to hospital within 30 days\n",
    "data.readmitted.value_counts()\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 0 if x == 'NO' or x == '>30'\n",
    "                                              else 1)\n",
    "\n",
    "######-----------Imputing Missing Values in Race by drawing randomly from the race in the observations------------#########\n",
    "\n",
    "# Checking the value counts of race and where values are missing \n",
    "races = data['race'].loc[data['race'] != '?'].values\n",
    "data['race'].value_counts()\n",
    "\n",
    "# Applying a lambda function to the race column to impute missing values \n",
    "data['race'] = data['race'].apply(lambda x: random.choice(races) if x == '?' else x)\n",
    "data['race'].value_counts()\n",
    "\n",
    "# Re-indexing\n",
    "data.index = np.arange(0,len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91a14e",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "*By Anastasia Wei*\n",
    "\n",
    "The following data preparation steps helped us to prepare our data for implementing various modeling / validation techniques:\n",
    "\n",
    "1. Since we need to predict house price, we derived some new predictors *(from existing predictors)* that intuitively seem to be helpuful to predict house price. \n",
    "\n",
    "2. We have shuffled the dataset to prepare it for K-fold cross validation.\n",
    "\n",
    "3. We have created a standardized version of the dataset, as we will use it to develop Lasso / Ridge regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b2b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######-----------Replacing the age range with the middle of the interval------------#########\n",
    "replaceDict = {'[0-10)' : 5,'[10-20)' : 15, '[20-30)' : 25, \n",
    "               '[30-40)' : 35,'[40-50)' : 45,'[50-60)' : 55,\n",
    "               '[60-70)' : 65,'[70-80)' : 75,'[80-90)' : 85,\n",
    "               '[90-100)' : 95}\n",
    "\n",
    "data['age'] = data['age'].apply(lambda x : replaceDict[x])\n",
    "print(data['age'].head())\n",
    "\n",
    "######-----------Using domain knowledge on diag_1, diag_2, and diag_3 to bin the diagnoses------------#########\n",
    "\n",
    "# Defining a helper function to determine if a number is a float\n",
    "def isfloat(num):\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Defining a function to bin the diagnoses columns\n",
    "def diag_transform(x):\n",
    "    if str(x)[0] == 'V' or str(x)[0] == 'E':\n",
    "        return 'other'\n",
    "    elif isfloat(x):\n",
    "        if int(float(x)) in range(390, 460) or int(float(x)) == 785:\n",
    "            return 'circulatory'\n",
    "        elif int(float(x)) in range(460, 520) or int(float(x)) == 786:\n",
    "            return 'respiratory'\n",
    "        elif int(float(x)) in range(520, 580) or int(float(x)) == 787:\n",
    "            return 'digestive'\n",
    "        elif int(float(x)) == 250:\n",
    "            return'diabetes'\n",
    "        elif int(float(x)) in range(800, 1000):\n",
    "            return 'injury'\n",
    "        elif int(float(x)) in range(710, 740):\n",
    "            return 'musculoskeletal'\n",
    "        elif int(float(x)) in range(580, 630) or int(float(x)) == 788:\n",
    "            return 'genitourinary'\n",
    "        elif int(float(x)) in range(140, 240):\n",
    "            return 'neoplasms'\n",
    "        elif int(float(x)) in range(630, 680):\n",
    "            return 'pregnecy'\n",
    "        else:\n",
    "            return 'other'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Applying the functions to the appropriate columns in the dataframe \n",
    "data['diag_1'] = data['diag_1'].apply(diag_transform)\n",
    "data['diag_2'] = data['diag_2'].apply(diag_transform)\n",
    "data['diag_3'] = data['diag_3'].apply(diag_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e04063",
   "metadata": {},
   "outputs": [],
   "source": [
    "######-----------Using domain knowledge to bin admission_type_id, discharge_disposition_id, admission_source_id------------#########\n",
    "\n",
    "# Changing the datatypes of the appropriate columns \n",
    "data['admission_type_id'] = data['admission_type_id'].astype('int')\n",
    "data['admission_source_id'] = data['admission_source_id'].astype('int')\n",
    "data['discharge_disposition_id'] = data['discharge_disposition_id'].astype('int')\n",
    "\n",
    "# Defining helper functions to transform the data \n",
    "def ad_type_transform(x):\n",
    "    if x in [2,7]:\n",
    "        return 1\n",
    "    elif x in [6,8]:\n",
    "        return 5\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def ad_source_transform(x):\n",
    "    if x in [2,3]:\n",
    "        return 1\n",
    "    elif x in [5,6,10,22,25]:\n",
    "        return 4\n",
    "    elif x in [15,17,20,21]:\n",
    "        return 9\n",
    "    elif x in [13,14]:\n",
    "        return 11\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def discharge_transform(x):\n",
    "    if x in [6, 8, 9, 13]:\n",
    "        return 1\n",
    "    elif x in [3, 4, 5, 14, 22, 23, 24]:\n",
    "        return 2\n",
    "    elif x in [12, 15, 16, 17]:\n",
    "        return 10\n",
    "    elif x in [19, 20, 21]:\n",
    "        return 11\n",
    "    elif x in [25, 26]:\n",
    "        return 18\n",
    "    elif x in [13,14,19,20,21]:\n",
    "        return 11\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Applying the helper functions to the appropriate columns\n",
    "data['admission_type_id'] = data['admission_type_id'].apply(ad_type_transform)\n",
    "data['admission_source_id'] = data['admission_source_id'].apply(ad_source_transform)\n",
    "data['discharge_disposition_id'] = data['discharge_disposition_id'].apply(discharge_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######-----Creating number of changes variable (number of 'ups' and 'downs' in medication)-------#########\n",
    "\n",
    "# Creating a list of the drugs in the dataset\n",
    "druglist = ['metformin','repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "            'glimepiride','acetohexamide', 'glipizide', 'glyburide', \n",
    "            'tolbutamide','pioglitazone', 'rosiglitazone', 'acarbose', \n",
    "            'miglitol', 'troglitazone','tolazamide', 'examide', 'citoglipton', \n",
    "            'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "            'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "            'metformin-pioglitazone']\n",
    "\n",
    "# Defining a helper function to help create this variable \n",
    "num_of_changes = []\n",
    "for i in tqdm(range(len(data))) :\n",
    "    changeCount = 0\n",
    "    for col in druglist : \n",
    "        if data.iloc[i][col] in ['Down', 'Up'] :\n",
    "            changeCount += 1\n",
    "    num_of_changes.append(changeCount)\n",
    "    \n",
    "# Creating the variable in the dataset\n",
    "data['num_of_changes'] = num_of_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "######-----Saving the cleaned data as a CSV-------#########\n",
    "data.to_csv('diabetes_cleaned.csv')\n",
    "\n",
    "######-----Creating test and train datasets-------#########\n",
    "\n",
    "# Using 80% for train, 20% for test\n",
    "print(len(data))\n",
    "print(len(data)*0.2)\n",
    "\n",
    "# Creating the test data\n",
    "pop = list(np.arange(0,71518))\n",
    "test_loc = random.sample(pop, k = 14304)\n",
    "test = data.iloc[test_loc]\n",
    "test.index = np.arange(0, 14304)\n",
    "\n",
    "# Creating the train data \n",
    "train_loc = list(set(pop) - set(test_loc))\n",
    "train = data.iloc[train_loc]\n",
    "train.index = np.arange(0, 57214)\n",
    "\n",
    "# Saving both dfs as CSVs\n",
    "test.to_csv('test_csv')\n",
    "train.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "*By Anastasia Wei, Lila Wells, Kaitlyn Hung, and Amy Wang*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd74a9",
   "metadata": {},
   "source": [
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd43a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Developing the model\n",
    "*By Anastasia Wei, Lila Wells, Kaitlyn Hung, and Amy Wang*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ec4c9",
   "metadata": {},
   "source": [
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2016145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6444e611",
   "metadata": {},
   "source": [
    "### Code fitting the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cae72f",
   "metadata": {},
   "source": [
    "Put the code(s) that fit the final model(s) in separate cell(s), i.e., the code with the `.ols()` or `.logit()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843163a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)\n",
    "*By Anastasia Wei, Lila Wells, Kaitlyn Hung, and Amy Wang*\n",
    "\n",
    "Put the odds-ratio calculations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8384a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
